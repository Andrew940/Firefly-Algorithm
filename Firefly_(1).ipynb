{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Firefly (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JainamS1996/Firefly-Algorithm/blob/master/Firefly_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ZWY-jJ8Z5WV",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import operator\n",
        "\n",
        "\n",
        "from numpy.random import random_sample\n",
        "\n",
        "__all__ = ['Ackley', 'Michalewicz']\n",
        "\n",
        "\n",
        "class BaseFunc:\n",
        "    def __init__(self, dim):\n",
        "        self.dim = dim\n",
        "        self.min_bound = np.zeros(self.dim)\n",
        "        self.max_bound = np.zeros(self.dim)\n",
        "        self.solution = np.zeros(self.dim)\n",
        "        self.global_optima = 0\n",
        "        self.plot_place = 0.25\n",
        "        self.m = 10\n",
        "        self.title = ''\n",
        "\n",
        "    def get_global_optima(self):\n",
        "        return self.global_optima\n",
        "\n",
        "    def get_solution(self):\n",
        "        return self.solution\n",
        "\n",
        "    def get_search_bounds(self):\n",
        "        return [self.min_bound, self.max_bound]\n",
        "\n",
        "    def get_y(self, x):\n",
        "        return -1\n",
        "\n",
        "    def plot(self):\n",
        "        x = np.arange(self.min_bound[0], self.max_bound[0], self.plot_place, dtype=np.float32)\n",
        "        y = np.arange(self.min_bound[1], self.max_bound[1], self.plot_place, dtype=np.float32)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        Z = []\n",
        "        for coord in zip(X, Y):\n",
        "            z = []\n",
        "            for input in zip(coord[0], coord[1]):\n",
        "                tmp = list(input)\n",
        "                tmp.extend(list(self.solution[0:self.dim - 2]))\n",
        "                z.append(self.get_y(np.array(tmp)))\n",
        "            Z.append(z)\n",
        "        Z = np.array(Z)\n",
        "        fig = plt.figure()\n",
        "        ax = Axes3D(fig)\n",
        "        ax.plot_wireframe(X, Y, Z)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class Ackley(BaseFunc):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super().__init__(dim)\n",
        "        self.max_bound = np.array([32.768] * self.dim)\n",
        "        self.min_bound = np.array([-32.768] * self.dim)\n",
        "        self.solution = np.ones(self.dim)\n",
        "        self.global_optima = 0\n",
        "        self.title = 'Ackley'\n",
        "\n",
        "    def get_y(self, x):\n",
        "        return 20. - 20. * np.exp(-0.2 * np.sqrt(1. / self.dim * np.sum(np.square(x)))) + np.e - np.exp(\n",
        "            1. / self.dim * np.sum(np.cos(x * 2. * np.pi)))\n",
        "\n",
        "    def get_y_2d(self, x, y):\n",
        "        return 20. - 20. * np.exp(-0.2 * np.sqrt(1. / self.dim * (x ** 2 + y ** 2))) + np.e - np.exp(\n",
        "            1. / self.dim * (np.cos(x * 2. * np.pi) + np.cos(y * 2. * np.pi)))\n",
        "\n",
        "\n",
        "class Michalewicz(BaseFunc):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super().__init__(dim)\n",
        "        self.max_bound = np.array([np.pi] * self.dim)\n",
        "        self.min_bound = np.zeros(self.dim)\n",
        "        self.solution = np.zeros(self.dim)\n",
        "        self.global_optima = self.get_y(self.solution)\n",
        "        self.title = 'Michalewicz'\n",
        "        self.m = 10\n",
        "\n",
        "    def get_y(self, x):\n",
        "        y = 0\n",
        "        for i in range(self.dim):\n",
        "            y += np.sin(x[i]) * np.power(np.sin((i + 1) * np.power(x[i], 2) / np.pi), 2 * self.m)\n",
        "        return -y\n",
        "\n",
        "    def get_y_2d(self, x, y):\n",
        "        yy = 0\n",
        "        yy += np.sin(x) * np.power(np.sin((0 + 1) * np.power(x, 2) / np.pi), 2 * self.m)\n",
        "        yy += np.sin(y) * np.power(np.sin((1 + 1) * np.power(y, 2) / np.pi), 2 * self.m)\n",
        "        return -yy\n",
        "\n",
        "    \n",
        "def generate_population(population_size, problem_dim, min_bound, max_bound):\n",
        "    predictedY = ga_model.predict(trainx)\n",
        "    error = np.mean((trainy_binary - predictedY)**2)\n",
        "    data = (max_bound + error - min_bound) * random_sample((population_size, problem_dim)) + min_bound\n",
        "    data[data > max_bound] = max_bound\n",
        "    return data\n",
        "    \n",
        "# TODO: refactor function firefly_optimisation function call\n",
        "class Firefly:\n",
        "\n",
        "    def __init__(self, problem_dim, min_bound, max_bound):\n",
        "        self.func = Michalewicz(problem_dim)\n",
        "        self.position = generate_population(1, problem_dim, min_bound, max_bound)[0]\n",
        "        self.brightness = None\n",
        "        self.update_brightness()\n",
        "\n",
        "    # the best fit is 0\n",
        "    def update_brightness(self):\n",
        "        self.brightness = -self.func.get_y(self.position)\n",
        "\n",
        "\n",
        "class FireflyOptimizer:\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.population_size = int(kwargs.get('population_size', 10))\n",
        "        self.problem_dim = kwargs.get('problem_dim', 2)\n",
        "        self.min_bound = kwargs.get('min_bound', -5)\n",
        "        self.max_bound = kwargs.get('max_bound', 5)\n",
        "        self.generations = kwargs.get('generations', 10)\n",
        "        self.population = self._population(self.population_size, self.problem_dim, self.min_bound, self.max_bound)\n",
        "        self.gamma = kwargs.get('gamma', 0.97)  # absorption coefficient\n",
        "        self.alpha = kwargs.get('alpha', 0.25)  # randomness [0,1]\n",
        "        self.beta_init = kwargs.get('beta_init', 1)\n",
        "        self.beta_min = kwargs.get('beta_min', 0.2)\n",
        "        self.optimization_benchmark = kwargs.get('optimization_benchmark', 'Ackley')\n",
        "    \n",
        "    \n",
        "    @staticmethod\n",
        "    \n",
        "    def _population(population_size, problem_dim, min_bound, max_bound):\n",
        "        population = []\n",
        "        for i in range(population_size):\n",
        "            population.append(Firefly(problem_dim, min_bound, max_bound))\n",
        "        return population\n",
        "\n",
        "    def step(self):\n",
        "        self.population.sort(key=operator.attrgetter('brightness'), reverse=True)\n",
        "        self._modify_alpha()\n",
        "        tmp_population = self.population\n",
        "        for i in range(self.population_size):\n",
        "            for j in range(self.population_size):\n",
        "                if self.population[i].brightness > tmp_population[j].brightness:\n",
        "                    r = math.sqrt(np.sum((self.population[i].position - tmp_population[j].position) ** 2))\n",
        "                    beta = (self.beta_init - self.beta_min) * math.exp(-self.gamma * r ** 2) + self.beta_min\n",
        "                    tmp = self.alpha * (np.random.random_sample((1, self.problem_dim))[0] - 0.5) * (\n",
        "                            self.max_bound - self.min_bound)\n",
        "                    self.population[j].position = self.check_position(\n",
        "                        self.population[i].position * (1 - beta) + tmp_population[\n",
        "                            j].position * beta + tmp)\n",
        "                    self.population[j].update_brightness()\n",
        "        self.population[0].position = generate_population(1, self.problem_dim, self.min_bound, self.max_bound)[0]\n",
        "        self.population[0].update_brightness()\n",
        "\n",
        "    def run_firefly(self):\n",
        "        for t in range(self.generations):\n",
        "            print('Generation %s, best fitness %s' % (t, self.population[0].brightness))\n",
        "            self.step()\n",
        "        self.population.sort(key=operator.attrgetter('brightness'), reverse=True)\n",
        "        a[]=self.population[0].brightness\n",
        "        return self.population[0].brightness, self.population[0].position\n",
        "    def check_position(self, position):\n",
        "        position[position > self.max_bound] = self.max_bound\n",
        "        position[position < self.min_bound] = self.min_bound\n",
        "        return position\n",
        "\n",
        "    def _modify_alpha(self):\n",
        "        delta = 1 - (10 ** (-4) / 0.9) ** (1 / self.generations)\n",
        "        self.alpha = (1 - delta) * self.alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eEsBVLtxeag6",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split as split\n",
        "\n",
        "from keras.layers import LSTM, Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense, TimeDistributed, Activation\n",
        "from keras import backend\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_eKM-R_XHr2F",
        "outputId": "d411c436-b8d0-4908-a4d1-3f5aa6822a03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df = pd.read_csv('clean_bank.csv',sep=',',header=0)\n",
        "df.shape\n",
        "df = df.iloc[4000:]\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(521, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nTDQ3ZsDHYyX",
        "outputId": "d6318f2d-2e73-47a3-dc0e-82b6c534660c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x=df.iloc[:,1:13]\n",
        "y=df.iloc[:,13]\n",
        "trainx,testx,trainy,testy=split(x,y,test_size=0.33)\n",
        "print(trainx.shape,testx.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(349, 12) (172, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cl-RV7SnFqze",
        "colab": {}
      },
      "source": [
        "trainx=trainx.values\n",
        "trainy=trainy.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I_YqKKM0q_W8",
        "outputId": "9709a380-c767-4cd4-9890-342e6b5cf336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3655
        }
      },
      "source": [
        "model = Sequential()\n",
        "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
        "# in the first layer, you must specify the expected input data shape:\n",
        "# here, 20-dimensional vectors.\n",
        "model.add(Dense(64, activation='relu', input_dim=12,name='layer_1'))\n",
        "model.add(Dense(64, activation='relu',name='layer_2'))\n",
        "model.add(Dense(2, activation='softmax',name='layer_3'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(trainx, trainy_binary,\n",
        "          epochs=100,\n",
        "          batch_size=128)\n",
        "score = model.evaluate(testx, testy_binary, batch_size=128)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "349/349 [==============================] - 0s 1ms/step - loss: 1.8916 - acc: 0.8682\n",
            "Epoch 2/100\n",
            "349/349 [==============================] - 0s 30us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 3/100\n",
            "349/349 [==============================] - 0s 45us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 4/100\n",
            "349/349 [==============================] - 0s 39us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 5/100\n",
            "349/349 [==============================] - 0s 49us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 6/100\n",
            "349/349 [==============================] - 0s 41us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 7/100\n",
            "349/349 [==============================] - 0s 49us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 8/100\n",
            "349/349 [==============================] - 0s 51us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 9/100\n",
            "349/349 [==============================] - 0s 39us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 10/100\n",
            "349/349 [==============================] - 0s 36us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 11/100\n",
            "349/349 [==============================] - 0s 39us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 12/100\n",
            "349/349 [==============================] - 0s 32us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 13/100\n",
            "349/349 [==============================] - 0s 46us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 14/100\n",
            "349/349 [==============================] - 0s 39us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 15/100\n",
            "349/349 [==============================] - 0s 57us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 16/100\n",
            "349/349 [==============================] - 0s 61us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 17/100\n",
            "349/349 [==============================] - 0s 46us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 18/100\n",
            "349/349 [==============================] - 0s 38us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 19/100\n",
            "349/349 [==============================] - 0s 37us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 20/100\n",
            "349/349 [==============================] - 0s 42us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 21/100\n",
            "349/349 [==============================] - 0s 28us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 22/100\n",
            "349/349 [==============================] - 0s 29us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 23/100\n",
            "349/349 [==============================] - 0s 50us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 24/100\n",
            "349/349 [==============================] - 0s 53us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 25/100\n",
            "349/349 [==============================] - 0s 49us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 26/100\n",
            "349/349 [==============================] - 0s 50us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 27/100\n",
            "349/349 [==============================] - 0s 40us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 28/100\n",
            "349/349 [==============================] - 0s 33us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 29/100\n",
            "349/349 [==============================] - 0s 52us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 30/100\n",
            "349/349 [==============================] - 0s 33us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 31/100\n",
            "349/349 [==============================] - 0s 34us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 32/100\n",
            "349/349 [==============================] - 0s 40us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 33/100\n",
            "349/349 [==============================] - 0s 37us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 34/100\n",
            "349/349 [==============================] - 0s 26us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 35/100\n",
            "349/349 [==============================] - 0s 34us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 36/100\n",
            "349/349 [==============================] - 0s 44us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 37/100\n",
            "349/349 [==============================] - 0s 48us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 38/100\n",
            "349/349 [==============================] - 0s 32us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 39/100\n",
            "349/349 [==============================] - 0s 39us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 40/100\n",
            "349/349 [==============================] - 0s 33us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 41/100\n",
            "349/349 [==============================] - 0s 32us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 42/100\n",
            "349/349 [==============================] - 0s 48us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 43/100\n",
            "349/349 [==============================] - 0s 38us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 44/100\n",
            "349/349 [==============================] - 0s 47us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 45/100\n",
            "349/349 [==============================] - 0s 38us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 46/100\n",
            "349/349 [==============================] - 0s 36us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 47/100\n",
            "349/349 [==============================] - 0s 40us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 48/100\n",
            "349/349 [==============================] - 0s 44us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 49/100\n",
            "349/349 [==============================] - 0s 50us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 50/100\n",
            "349/349 [==============================] - 0s 49us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 51/100\n",
            "349/349 [==============================] - 0s 57us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 52/100\n",
            "349/349 [==============================] - 0s 45us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 53/100\n",
            "349/349 [==============================] - 0s 41us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 54/100\n",
            "349/349 [==============================] - 0s 59us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 55/100\n",
            "349/349 [==============================] - 0s 48us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 56/100\n",
            "349/349 [==============================] - 0s 55us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 57/100\n",
            "349/349 [==============================] - 0s 62us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 58/100\n",
            "349/349 [==============================] - 0s 47us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 59/100\n",
            "349/349 [==============================] - 0s 42us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 60/100\n",
            "349/349 [==============================] - 0s 42us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 61/100\n",
            "349/349 [==============================] - 0s 36us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 62/100\n",
            "349/349 [==============================] - 0s 36us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 63/100\n",
            "349/349 [==============================] - 0s 28us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 64/100\n",
            "349/349 [==============================] - 0s 38us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 65/100\n",
            "349/349 [==============================] - 0s 49us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 66/100\n",
            "349/349 [==============================] - 0s 36us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 67/100\n",
            "349/349 [==============================] - 0s 55us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 68/100\n",
            "349/349 [==============================] - 0s 42us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 69/100\n",
            "349/349 [==============================] - 0s 44us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 70/100\n",
            "349/349 [==============================] - 0s 44us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 71/100\n",
            "349/349 [==============================] - 0s 35us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 72/100\n",
            "349/349 [==============================] - 0s 36us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 73/100\n",
            "349/349 [==============================] - 0s 55us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 74/100\n",
            "349/349 [==============================] - 0s 55us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 75/100\n",
            "349/349 [==============================] - 0s 54us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 76/100\n",
            "349/349 [==============================] - 0s 47us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 77/100\n",
            "349/349 [==============================] - 0s 54us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 78/100\n",
            "349/349 [==============================] - 0s 36us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 79/100\n",
            "349/349 [==============================] - 0s 43us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 80/100\n",
            "349/349 [==============================] - 0s 37us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 81/100\n",
            "349/349 [==============================] - 0s 39us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 82/100\n",
            "349/349 [==============================] - 0s 29us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 83/100\n",
            "349/349 [==============================] - 0s 37us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 84/100\n",
            "349/349 [==============================] - 0s 37us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 85/100\n",
            "349/349 [==============================] - 0s 35us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 86/100\n",
            "349/349 [==============================] - 0s 36us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 87/100\n",
            "349/349 [==============================] - 0s 39us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 88/100\n",
            "349/349 [==============================] - 0s 43us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 89/100\n",
            "349/349 [==============================] - 0s 87us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 90/100\n",
            "349/349 [==============================] - 0s 61us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 91/100\n",
            "349/349 [==============================] - 0s 48us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 92/100\n",
            "349/349 [==============================] - 0s 60us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 93/100\n",
            "349/349 [==============================] - 0s 60us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 94/100\n",
            "349/349 [==============================] - 0s 44us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 95/100\n",
            "349/349 [==============================] - 0s 56us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 96/100\n",
            "349/349 [==============================] - 0s 49us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 97/100\n",
            "349/349 [==============================] - 0s 31us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 98/100\n",
            "349/349 [==============================] - 0s 33us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 99/100\n",
            "349/349 [==============================] - 0s 39us/step - loss: 1.7088 - acc: 0.8940\n",
            "Epoch 100/100\n",
            "349/349 [==============================] - 0s 53us/step - loss: 1.7088 - acc: 0.8940\n",
            "172/172 [==============================] - 0s 2ms/step\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_1 (Dense)              (None, 64)                832       \n",
            "_________________________________________________________________\n",
            "layer_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "layer_3 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 5,122\n",
            "Trainable params: 5,122\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jpzktiKMFtUw",
        "colab": {}
      },
      "source": [
        "trainy_binary = to_categorical(trainy)\n",
        "testy_binary = to_categorical(testy)\n",
        "print(trainy_binary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "880MLF75zhXg",
        "colab": {}
      },
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
        "\n",
        "def r_square_loss(y_true, y_pred):\n",
        "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
        "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
        "    return 1 - ( 1 - SS_res/(SS_tot + K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcixQ7Qo3wie",
        "colab_type": "code",
        "outputId": "56c493d9-3eaa-44f9-a140-7e8761fbb4cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.4364563531653824, 0.848837207916171]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MK3jN_bBrHPc",
        "outputId": "d73dac41-328f-4669-839e-c6b7c6ee182a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "weights_layer1 = model.layers[0].get_weights()[0]\n",
        "biases_layer1 = model.layers[0].get_weights()[1]\n",
        "\n",
        "weights_layer2 = model.layers[1].get_weights()[0]\n",
        "biases_layer2 = model.layers[1].get_weights()[1]\n",
        "\n",
        "weights_layer3 = model.layers[2].get_weights()[0]\n",
        "biases_layer3 = model.layers[2].get_weights()[1]\n",
        "\n",
        "print(np.shape(weights_layer1))\n",
        "print(np.shape(biases_layer1))\n",
        "print(np.shape(weights_layer2))\n",
        "print(np.shape(biases_layer2))\n",
        "print(np.shape(weights_layer3))\n",
        "print(np.shape(biases_layer3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12, 64)\n",
            "(64,)\n",
            "(64, 64)\n",
            "(64,)\n",
            "(64, 2)\n",
            "(2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cgKIZ6Bz6NeN",
        "colab": {}
      },
      "source": [
        "ga_model = Sequential()\n",
        "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
        "# in the first layer, you must specify the expected input data shape:\n",
        "# here, 20-dimensional vectors.\n",
        "ga_model.add(Dense(64, activation='relu', input_dim=12,name='layer_1'))\n",
        "ga_model.add(Dense(64, activation='relu',name='layer_2'))\n",
        "ga_model.add(Dense(2, activation='softmax',name='layer_3'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "ga_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yd9f-F-Q58SG",
        "outputId": "54ebddc3-3a04-466b-fb8a-174a3bc83138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3485
        }
      },
      "source": [
        "first_layer = ga_model.get_layer('layer_1').get_weights()\n",
        "second_layer = ga_model.get_layer('layer_2').get_weights()\n",
        "third_layer=ga_model.get_layer('layer_3').get_weights()\n",
        "dimensions = (first_layer[0].shape[0] * first_layer[0].shape[1]) + (first_layer[1].shape[0]) + (second_layer[0].shape[0] * second_layer[0].shape[1]) +(second_layer[1].shape[0])+(third_layer[0].shape[0] * third_layer[0].shape[1]) +(third_layer[1].shape[0])\n",
        "print(\"Number of params in Neural Network: %d\" % dimensions)\n",
        "\n",
        "optimizer = FireflyOptimizer(population_size=25, problem_dim=dimensions, generations=200, beta_min=0.65, alpha=0.05)\n",
        "optimizer.run_firefly()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of params in Neural Network: 5122\n",
            "Generation 0, best fitness -85.16611768182382\n",
            "Generation 1, best fitness -60.9299954418877\n",
            "Generation 2, best fitness -53.943469943582144\n",
            "Generation 3, best fitness -58.069283445749065\n",
            "Generation 4, best fitness -88.36110396182877\n",
            "Generation 5, best fitness -78.13870230602026\n",
            "Generation 6, best fitness -102.35557685825097\n",
            "Generation 7, best fitness -69.25458886169973\n",
            "Generation 8, best fitness -53.222446460770826\n",
            "Generation 9, best fitness -63.54054883588625\n",
            "Generation 10, best fitness -75.46930634574544\n",
            "Generation 11, best fitness -49.771603513182875\n",
            "Generation 12, best fitness -66.48247069978305\n",
            "Generation 13, best fitness -91.01380954661145\n",
            "Generation 14, best fitness -52.99090983232552\n",
            "Generation 15, best fitness -73.98690333687217\n",
            "Generation 16, best fitness -88.34384950092813\n",
            "Generation 17, best fitness -70.69725608713978\n",
            "Generation 18, best fitness -100.92782871497074\n",
            "Generation 19, best fitness -48.69892350902817\n",
            "Generation 20, best fitness -42.614065441066415\n",
            "Generation 21, best fitness -46.810583641130094\n",
            "Generation 22, best fitness -82.2792999134065\n",
            "Generation 23, best fitness -80.19385524697643\n",
            "Generation 24, best fitness -44.04357094770461\n",
            "Generation 25, best fitness -82.34981341750826\n",
            "Generation 26, best fitness -45.9476610080946\n",
            "Generation 27, best fitness -41.769411186889535\n",
            "Generation 28, best fitness -50.394871010296306\n",
            "Generation 29, best fitness -33.791503769815435\n",
            "Generation 30, best fitness -36.41819282386866\n",
            "Generation 31, best fitness -44.52429003192866\n",
            "Generation 32, best fitness -31.72782021281112\n",
            "Generation 33, best fitness -46.13493261174204\n",
            "Generation 34, best fitness -48.21846303851991\n",
            "Generation 35, best fitness -89.37439427267093\n",
            "Generation 36, best fitness -47.94137659011916\n",
            "Generation 37, best fitness -95.80857402080623\n",
            "Generation 38, best fitness -50.442006883570315\n",
            "Generation 39, best fitness -12.720238928966017\n",
            "Generation 40, best fitness -59.05879780265186\n",
            "Generation 41, best fitness -63.86323584757983\n",
            "Generation 42, best fitness -70.30397173572955\n",
            "Generation 43, best fitness -56.67088134441529\n",
            "Generation 44, best fitness -45.74944518276661\n",
            "Generation 45, best fitness -62.48341449221591\n",
            "Generation 46, best fitness -79.43150596157017\n",
            "Generation 47, best fitness -82.41882886762518\n",
            "Generation 48, best fitness -79.02915134390553\n",
            "Generation 49, best fitness -47.05937122701219\n",
            "Generation 50, best fitness -65.16232236460468\n",
            "Generation 51, best fitness -25.553143699272677\n",
            "Generation 52, best fitness -49.90830707881879\n",
            "Generation 53, best fitness -65.7193724405454\n",
            "Generation 54, best fitness -53.298875099404235\n",
            "Generation 55, best fitness -42.52355871791022\n",
            "Generation 56, best fitness -53.44701300280627\n",
            "Generation 57, best fitness -24.117945829351974\n",
            "Generation 58, best fitness -81.29974741631868\n",
            "Generation 59, best fitness -72.38040054221817\n",
            "Generation 60, best fitness -54.86930532279252\n",
            "Generation 61, best fitness -33.414837274770434\n",
            "Generation 62, best fitness -73.7678070515525\n",
            "Generation 63, best fitness -76.95439969114429\n",
            "Generation 64, best fitness -29.62486388162881\n",
            "Generation 65, best fitness -71.38680660194727\n",
            "Generation 66, best fitness -64.24541520602324\n",
            "Generation 67, best fitness -49.607827518963916\n",
            "Generation 68, best fitness -72.82901492848636\n",
            "Generation 69, best fitness -35.275691366113804\n",
            "Generation 70, best fitness -58.48089038319995\n",
            "Generation 71, best fitness -86.19136819739975\n",
            "Generation 72, best fitness -63.32556821660334\n",
            "Generation 73, best fitness -47.687123588830204\n",
            "Generation 74, best fitness -41.72835756155434\n",
            "Generation 75, best fitness -56.694725298652294\n",
            "Generation 76, best fitness -50.1564951204076\n",
            "Generation 77, best fitness -64.20973939744746\n",
            "Generation 78, best fitness -54.393031376444554\n",
            "Generation 79, best fitness -22.32614490793315\n",
            "Generation 80, best fitness -33.74065650052484\n",
            "Generation 81, best fitness -72.60359426123826\n",
            "Generation 82, best fitness -40.661238759050036\n",
            "Generation 83, best fitness -28.062834625574\n",
            "Generation 84, best fitness -65.49179435666804\n",
            "Generation 85, best fitness -47.63531982613226\n",
            "Generation 86, best fitness -53.840055803938455\n",
            "Generation 87, best fitness -67.92315852824078\n",
            "Generation 88, best fitness -16.71886639153918\n",
            "Generation 89, best fitness -57.85333297231042\n",
            "Generation 90, best fitness -47.936359911660574\n",
            "Generation 91, best fitness -84.79644904390231\n",
            "Generation 92, best fitness -53.841386359633304\n",
            "Generation 93, best fitness -38.69435493433649\n",
            "Generation 94, best fitness -60.398867467940725\n",
            "Generation 95, best fitness -49.684171493734475\n",
            "Generation 96, best fitness -51.149321755568444\n",
            "Generation 97, best fitness -30.534995481620587\n",
            "Generation 98, best fitness -73.5366588547352\n",
            "Generation 99, best fitness -69.41753323304556\n",
            "Generation 100, best fitness -81.7033678749659\n",
            "Generation 101, best fitness -81.37275592481171\n",
            "Generation 102, best fitness -49.39022080146926\n",
            "Generation 103, best fitness -60.86850257644536\n",
            "Generation 104, best fitness -47.67975116004792\n",
            "Generation 105, best fitness -36.67346718942788\n",
            "Generation 106, best fitness -44.67113218846186\n",
            "Generation 107, best fitness -75.81049580273454\n",
            "Generation 108, best fitness -41.92959920242835\n",
            "Generation 109, best fitness -83.4210736889123\n",
            "Generation 110, best fitness -44.66494074154387\n",
            "Generation 111, best fitness -44.627039345219806\n",
            "Generation 112, best fitness -39.23550282178408\n",
            "Generation 113, best fitness -67.3328053486918\n",
            "Generation 114, best fitness -58.61558511345476\n",
            "Generation 115, best fitness -81.76599429359491\n",
            "Generation 116, best fitness -42.25086232467109\n",
            "Generation 117, best fitness -40.04797382253637\n",
            "Generation 118, best fitness -19.323591281121132\n",
            "Generation 119, best fitness -52.13920719931585\n",
            "Generation 120, best fitness -75.77511794017569\n",
            "Generation 121, best fitness -37.05899869162088\n",
            "Generation 122, best fitness -66.51612889638047\n",
            "Generation 123, best fitness -35.26493074442817\n",
            "Generation 124, best fitness -57.16053891575834\n",
            "Generation 125, best fitness -35.13018702796541\n",
            "Generation 126, best fitness -87.4692805001923\n",
            "Generation 127, best fitness -43.88693658976545\n",
            "Generation 128, best fitness -69.20864776519942\n",
            "Generation 129, best fitness -56.995475971666494\n",
            "Generation 130, best fitness -75.32197305506749\n",
            "Generation 131, best fitness -70.01859578339212\n",
            "Generation 132, best fitness -50.588753557156856\n",
            "Generation 133, best fitness -57.729477378790975\n",
            "Generation 134, best fitness -58.164672135074504\n",
            "Generation 135, best fitness -62.501813907478294\n",
            "Generation 136, best fitness -65.80621076112345\n",
            "Generation 137, best fitness -88.2397705596792\n",
            "Generation 138, best fitness -65.31975936752848\n",
            "Generation 139, best fitness -53.92225759088534\n",
            "Generation 140, best fitness -40.463163807464106\n",
            "Generation 141, best fitness -68.81730484029018\n",
            "Generation 142, best fitness -78.95759421630109\n",
            "Generation 143, best fitness -56.787592417503596\n",
            "Generation 144, best fitness -78.25082640176804\n",
            "Generation 145, best fitness -81.30118882163673\n",
            "Generation 146, best fitness -56.93125157625851\n",
            "Generation 147, best fitness -59.40214819461524\n",
            "Generation 148, best fitness -69.21290570602491\n",
            "Generation 149, best fitness -59.36077232431364\n",
            "Generation 150, best fitness -68.79223227162602\n",
            "Generation 151, best fitness -80.00635398347845\n",
            "Generation 152, best fitness -81.56840046038295\n",
            "Generation 153, best fitness -73.8534385853048\n",
            "Generation 154, best fitness -21.81150548691393\n",
            "Generation 155, best fitness -62.87678066354732\n",
            "Generation 156, best fitness -56.07226551173653\n",
            "Generation 157, best fitness -95.64477894245806\n",
            "Generation 158, best fitness -48.22610938387669\n",
            "Generation 159, best fitness -60.51481054113064\n",
            "Generation 160, best fitness -57.1292577216308\n",
            "Generation 161, best fitness -36.091327050784734\n",
            "Generation 162, best fitness -82.33759355036135\n",
            "Generation 163, best fitness -58.683624832054896\n",
            "Generation 164, best fitness -54.320621176852406\n",
            "Generation 165, best fitness -30.72601520505253\n",
            "Generation 166, best fitness -70.15898795709624\n",
            "Generation 167, best fitness -81.72822190874524\n",
            "Generation 168, best fitness -60.091753792940146\n",
            "Generation 169, best fitness -72.34073022388787\n",
            "Generation 170, best fitness -59.218887252304356\n",
            "Generation 171, best fitness -89.90556726433098\n",
            "Generation 172, best fitness -64.4602642019345\n",
            "Generation 173, best fitness -66.97961190930661\n",
            "Generation 174, best fitness -47.165565799197715\n",
            "Generation 175, best fitness -90.21109567931522\n",
            "Generation 176, best fitness -47.096248134889436\n",
            "Generation 177, best fitness -75.3031859410711\n",
            "Generation 178, best fitness -58.519773068901536\n",
            "Generation 179, best fitness -63.82951437319816\n",
            "Generation 180, best fitness -26.389761381288043\n",
            "Generation 181, best fitness -87.43486284734558\n",
            "Generation 182, best fitness -58.84873393525641\n",
            "Generation 183, best fitness -59.912860354747686\n",
            "Generation 184, best fitness -40.822010793033066\n",
            "Generation 185, best fitness -96.03419950576661\n",
            "Generation 186, best fitness -94.74056094149229\n",
            "Generation 187, best fitness -79.13519830247515\n",
            "Generation 188, best fitness -56.736128144819425\n",
            "Generation 189, best fitness -47.65589464660355\n",
            "Generation 190, best fitness -41.91407650732981\n",
            "Generation 191, best fitness -24.72143053362212\n",
            "Generation 192, best fitness -51.09659738735029\n",
            "Generation 193, best fitness -56.455753747815926\n",
            "Generation 194, best fitness -47.284672634320785\n",
            "Generation 195, best fitness -39.973314206878584\n",
            "Generation 196, best fitness -28.235818293728606\n",
            "Generation 197, best fitness -44.36871077129181\n",
            "Generation 198, best fitness -37.28217230018227\n",
            "Generation 199, best fitness -61.668596593891344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(266.78009811265144,\n",
              " array([ 0.32725945,  0.28874117, -0.36118012, ...,  0.41926962,\n",
              "        -0.35005179,  0.54773039]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YMrETU2ZzJyt",
        "colab": {}
      },
      "source": [
        "''''import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "__all__ = ['Ackley', 'Michalewicz']\n",
        "\n",
        "\n",
        "class BaseFunc:\n",
        "    def __init__(self, dim):\n",
        "        self.dim = dim\n",
        "        self.min_bound = np.zeros(self.dim)\n",
        "        self.max_bound = np.zeros(self.dim)\n",
        "        self.solution = np.zeros(self.dim)\n",
        "        self.global_optima = 0\n",
        "        self.plot_place = 0.25\n",
        "        self.m = 10\n",
        "        self.title = ''\n",
        "\n",
        "    def get_global_optima(self):\n",
        "        return self.global_optima\n",
        "\n",
        "    def get_solution(self):\n",
        "        return self.solution\n",
        "\n",
        "    def get_search_bounds(self):\n",
        "        return [self.min_bound, self.max_bound]\n",
        "\n",
        "    def get_y(self, x):\n",
        "        return -1\n",
        "\n",
        "    def plot(self):\n",
        "        x = np.arange(self.min_bound[0], self.max_bound[0], self.plot_place, dtype=np.float32)\n",
        "        y = np.arange(self.min_bound[1], self.max_bound[1], self.plot_place, dtype=np.float32)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        Z = []\n",
        "        for coord in zip(X, Y):\n",
        "            z = []\n",
        "            for input in zip(coord[0], coord[1]):\n",
        "                tmp = list(input)\n",
        "                tmp.extend(list(self.solution[0:self.dim - 2]))\n",
        "                z.append(self.get_y(np.array(tmp)))\n",
        "            Z.append(z)\n",
        "        Z = np.array(Z)\n",
        "        fig = plt.figure()\n",
        "        ax = Axes3D(fig)\n",
        "        ax.plot_wireframe(X, Y, Z)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class Ackley(BaseFunc):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super().__init__(dim)\n",
        "        self.max_bound = np.array([32.768] * self.dim)\n",
        "        self.min_bound = np.array([-32.768] * self.dim)\n",
        "        self.solution = np.ones(self.dim)\n",
        "        self.global_optima = 0\n",
        "        self.title = 'Ackley'\n",
        "\n",
        "    def get_y(self, x):\n",
        "        return 20. - 20. * np.exp(-0.2 * np.sqrt(1. / self.dim * np.sum(np.square(x)))) + np.e - np.exp(\n",
        "            1. / self.dim * np.sum(np.cos(x * 2. * np.pi)))\n",
        "\n",
        "    def get_y_2d(self, x, y):\n",
        "        return 20. - 20. * np.exp(-0.2 * np.sqrt(1. / self.dim * (x ** 2 + y ** 2))) + np.e - np.exp(\n",
        "            1. / self.dim * (np.cos(x * 2. * np.pi) + np.cos(y * 2. * np.pi)))\n",
        "\n",
        "\n",
        "class Michalewicz(BaseFunc):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super().__init__(dim)\n",
        "        self.max_bound = np.array([np.pi] * self.dim)\n",
        "        self.min_bound = np.zeros(self.dim)\n",
        "        self.solution = np.zeros(self.dim)\n",
        "        self.global_optima = self.get_y(self.solution)\n",
        "        self.title = 'Michalewicz'\n",
        "        self.m = 10\n",
        "\n",
        "    def get_y(self, x):\n",
        "        y = 0\n",
        "        for i in range(self.dim):\n",
        "            y += np.sin(x[i]) * np.power(np.sin((i + 1) * np.power(x[i], 2) / np.pi), 2 * self.m)\n",
        "        return -y\n",
        "\n",
        "    def get_y_2d(self, x, y):\n",
        "        yy = 0\n",
        "        yy += np.sin(x) * np.power(np.sin((0 + 1) * np.power(x, 2) / np.pi), 2 * self.m)\n",
        "        yy += np.sin(y) * np.power(np.sin((1 + 1) * np.power(y, 2) / np.pi), 2 * self.m)\n",
        "        return -yy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B4KVoI539gyy",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}